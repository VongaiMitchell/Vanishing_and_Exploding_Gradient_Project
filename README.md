# Vanishing_and_Exploding_Gradient_Project

Vanishing and Exploding gradients are common problems that occur during the training of deep neural networks, particularly those with many layers. The issue arises when gradients become too small or too large, making it difficult for the network to converge to anoptimal solution. 

This group project focuses on providing an overview of these issues, their impact on the process of model training, and common solutions used to resolve them.

Contributors:
Vongai Mitchell Makuwaza
Bahati Kilongo
Fay Elhassan
Solafa Fadallah
